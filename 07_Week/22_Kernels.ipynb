{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernels I\n",
    "\n",
    "- Not shown in the this cell, but one of the obstacles when segmenting the classes, we cannot always find a line that divides the classes as we should.\n",
    "- However, we could use a different type of seperation method: kernels (Gaussian Kernel)\n",
    "- The images below described if we create landmarks... these landmarks are then calculated to find the similarities of our inputs\n",
    "- The equation at the right is broken into two parts\n",
    "    1. The numerator calculates the Euclidean distance between the value and the landmark\n",
    "    2. The denonimator calculates the kernel that our equation will \"live in\"\n",
    "<img src=\"../images/Kernel.png\" alt=\"Drawing\" style=\"width: 400px;\">\n",
    "\n",
    "If we look at the diagram below, if the inputs are close to the inputs, our value will be 1. If not, the calculation will be closed to 0. These new calculation are new features that we create which will be used.\n",
    "<img src=\"../images/Kernel-2.png\" alt=\"Drawing\" style=\"width: 400px;\">\n",
    "<img src=\"../images/andrew_05.png\" alt=\"Drawing\" style=\"width: 400px;\">\n",
    "\n",
    "If we do plot the f function (in a third dimension), the closer the value is to 1, the higher it is on the plot. Hence, the closer the value to 0, the lower on the \"ground\" it is. \n",
    "<img src=\"../images/andrew_06.png\" alt=\"Drawing\" style=\"width: 400px;\">\n",
    "\n",
    "- Notice the $\\sigma^2$ plays an important role in how the shape of the \"mountain\" in the graph is! The higher the number, the more flat the mountain is.\n",
    "<img src=\"../images/andrew_07.png\" alt=\"Drawing\" style=\"width: 400px;\">\n",
    "\n",
    "We are creating this because we not divide/segment the data with a perfect line. We are projecting the data into a new dimension which provides a better way to segment the data. Using the landmarks, we can find/discover how our information classifies the data. If for example, the data close to landmark 1 and 2 predict 1, that's what our data will optimize for.\n",
    "<img src=\"../images/andrew_08.png\" alt=\"Drawing\" style=\"width: 400px;\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/alexguanga/All_Projects/exponential_learning/AndrewNgMachineLearning/07_Week\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernels II\n",
    "- How are the landmarks decided?\n",
    "    - The total number of observations should be equal to the total number of landmarks\n",
    "    - One landmark per observations\n",
    "    - Using the similarity function described above, we can now represent our data with the the f inputs\n",
    "    - <img src=\"../images/andrew_09.png\" alt=\"Drawing\" style=\"width: 500px;\">\n",
    "\n",
    "SVM Parameters\n",
    "- $C = 1/\\lambda$\n",
    "     - Large C (or small $\\lambda$) means lower bias but higher variance\n",
    "     - Small C (or large $\\lambda$) means higher bias but lower variance\n",
    "- $\\sigma^{2}$\n",
    "    - A large $\\sigma^{2}$ has a higher bias and low variance. \n",
    "    - A small $\\sigma^{2}$ has a lower bias and higher variance. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
