{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large Margin Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Objective\n",
    "- Support Vector Machine (SVM)\n",
    "- This is very similar to the Logisic Regression\n",
    "- For a cost function, we have a convex graph. However, for SVM, we are going to approximate the cost function with values equal to 0 until X=1, and then a linear line.\n",
    "- Look at the graphs below\n",
    "<img src=\"../images/Logistic-Compare.png\" alt=\"Drawing\" style=\"width: 500px;\">\n",
    "<img src=\"../images/SPV-1.png\" alt=\"Drawing\" style=\"width: 500px;\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large Margin Intuiton \n",
    "- Unlike the logistic regression, where the treshold was content with 0, the SVM rewards you if these cost $\\theta_TX$ is more than 1 or less than -1. \n",
    "- Then, the cost function would be 0.\n",
    "<img src=\"../images/SPV-2.png\" alt=\"Drawing\" style=\"width: 500px;\">\n",
    "<img src=\"../images/SPV-3.png\" alt=\"Drawing\" style=\"width: 500px;\">\n",
    "<img src=\"../images/SPV-4.png\"alt=\"Drawing\" style=\"width: 500px;\">\n",
    "\n",
    "- When we look at the decision boundaries, the optimally you want to divide the classification is with the largest margin possible. Meaning, you want the information to be divided as far as possible from the obersevations.\n",
    "\n",
    "- Instead of lambda, the C is the regularization parameter\n",
    "    - Thus, if C is very large, it's very sensitive to outliers.\n",
    "    - However, if C is small, the margin will separate the classes disregarding the outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematics Behind Large Margin Classification\n",
    "\n",
    "If we look at the graph below, we have two vectors (u and v)\n",
    "- We are projecting the v onto $u^T$. This is the same as the projection * the magnitude of u\n",
    "- <img src=\"../images/Math-SVM.png\" alt=\"Drawing\" style=\"width: 300px;\">\n",
    "\n",
    "Instead of u and v, we are substituing $u^T$ -> $\\theta^T$ & v -> x\n",
    "- <img src=\"../images/Math-SVM2.png\" alt=\"Drawing\" style=\"width: 300px;\">\n",
    "\n",
    "Another thing to keep in mind is that the decision boundary and the $\\theta$ are orthogonal.\n",
    "- Green line is the decision boundary (or will be)\n",
    "- <img src=\"../images/andrew_04.png\" alt=\"Drawing\" style=\"width: 300px;\">\n",
    "\n",
    "However, we need to find a better classification margin\n",
    "- The closer the values are to the classification margin, the higher the coefficients needs to be which is not something our model should optimize for\n",
    "<img src=\"../images/andrew_03.png\" alt=\"Drawing\" style=\"width: 500px;\">\n",
    "\n",
    "**The whole objective is to increase the margin between the data points**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
