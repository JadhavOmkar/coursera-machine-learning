{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large Margin Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Objective\n",
    "- Support Vector Machine (SPV)\n",
    "- This is very similar to the Logisic Regression\n",
    "- However, instead of looking at the curves, we are approximating the curves with two straight lines\n",
    "- Look at the graphs below\n",
    "<img src=\"../images/Logistic-Compare.png\" alt=\"Drawing\" style=\"width: 700px;\">\n",
    "<img src=\"../images/SPV-1.png\" alt=\"Drawing\" style=\"width: 700px;\">\n",
    "<img src=\"../images/SPV-2.png\" alt=\"Drawing\" style=\"width: 700px;\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large Margin Intuiton \n",
    "- Unlike the logistic regression, where the treshold was content with 0, the SPV rewards you if these cost Theta^TransposeX is more than 1 or less than -1. \n",
    "- Then, the cost function would be 0.\n",
    "<img src=\"../images/SPV-3.png\" alt=\"Drawing\" style=\"width: 500px;\">\n",
    "<img src=\"../images/SPV-4.png\"alt=\"Drawing\" style=\"width: 500px;\">\n",
    "\n",
    "- When we look at the decision boundaries, the optimally you want to divide the classification is with the largest margin possible. Meaning, you want the information to be divided as far as possible from the obersevations.\n",
    "\n",
    "- Instead of lambda, the C is the regularization parameter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematics Behind Large Margin Classification\n",
    "- You have two vectors. If you look at the diagrams, we are sort of projecting one vector to another but the vector projeced is orthogonal\n",
    "<img src=\"../images/Math-SVM.png\" alt=\"Drawing\" style=\"width: 500px;\">\n",
    "<img src=\"../images/Math-SVM2.png\" alt=\"Drawing\" style=\"width: 500px;\">\n",
    "\n",
    "\n",
    "- We need to look at the classifier and see the projection of the observation onto it. If we pay attention to the inner products, we know that the graph follows the similar idea. Check the graph below to get a better feel.\n",
    "<img src=\"../images/SPV-6.png\" alt=\"Drawing\" style=\"width: 500px;\">\n",
    "<img src=\"../images/SPV-7.png\"alt=\"Drawing\" style=\"width: 500px;\">\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
