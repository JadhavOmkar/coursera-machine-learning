{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Features\n",
    "- Can have multiple features\n",
    "- Linear regression with multiple variables is also known as \"multivariate linear regression\".\n",
    "- m = the number of training examples\n",
    "- n = the number of feature\n",
    "- $hθ_{1}=θ_{0}+θ_{1}x_{1}+θ_{2}x_{2}+θ_{3}x_{3}+⋯+θ_{n}x_{n}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent in Practice I\n",
    "- $\\theta$ (Theta) are the parameters\n",
    "- Feature Scaling: \n",
    "    - For the inputs, if their range, scale, are similar, the algorithm can be conducted quickly. \n",
    "    - If you plot two inputs that differ in scaling, we can see in a coutour map that that the eclipse is quite narrow and will take a long time until it gets to the middle\n",
    "    - **Solving for parameters that differ in scaling**: \n",
    "        - This can be fixed by scaling it or dividing it by the maximum possibility     \n",
    "        - You should try the feature into approximately from -1 to 1 (not required)\n",
    "        - Thumb of rule: Andrew likes ranges from -3 to 3, and -1/3 to 1/3\n",
    "        - **Mean normalization:** Replace $x_i$ with $x_i$ - mean to compute features to have approx. a mean of zero and this is divided either by the number of ranges of numbers or the std. deviation\n",
    "    - This is because $\\theta$ will descend quickly on small ranges and slowly on large ranges, and so will oscillate inefficiently down to the optimum when the variables are very uneven.\n",
    "\n",
    "\n",
    "Two techniques to help with this are feature scaling and mean normalization.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent in Practice II\n",
    "- Feature scaling involves dividing the input values by the range (i.e. the maximum value minus the minimum value) of the input variable, resulting in a new range of just 1. \n",
    "    - Mean normalization involves subtracting the average value for an input variable from the values for that input variable resulting in a new average value for the input variable of just zero.\n",
    "- *Debugging* - How to make that gradient descent is working correctly\n",
    "    - Kind of obvious, but J($\\theta$) should decrease when increasing the number of iterations\n",
    "    - If the gradient descent is increasing, then we probably need to decrease the learning rate\n",
    "    - For sufficient small alpha, J($\\theta$) should decrease after iteration but the $\\alpha$ shouldn't be too small.\n",
    "- **NOTE**: Use different alphas, and check which alpha fits best!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features and Polynomial Regression\n",
    "- Creating new variables. \n",
    "    - For examples, instead of using frontage and depth, you could use the area \n",
    "    - You can fit better models by either squaring or cubicing the variables\n",
    "    - One thing to keep in mind is that the ranges might eventually become too large \n",
    "    - We can change the behavior or curve of our hypothesis function by making it a quadratic, cubic or square root function (or any other form).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
