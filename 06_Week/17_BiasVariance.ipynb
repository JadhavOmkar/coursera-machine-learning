{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias and Variance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnosing Bias vs. Variance\n",
    "- **LEFT**: This is high bias problem. We are underfitting the model from the training to the cv or testing set. \n",
    "- **RIGHT**: This is high variance problem. We are overfitting the model from the training to the cv or testing set. \n",
    "- **BIAS**: The training and cv error will have high cost function\n",
    "- **VARIANCE**: The training error will be low (you are fitting the training set well), and the cv error will be higher than the training set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization and Bias/Variance\n",
    "\n",
    "- As we increase lambda, we are increasing the regurlaization term. Moreover, the coefficient decrease as we increase the lambda. Hence, the larger the lambda, the more regurlization takes into affect.\n",
    "- The difficulty is finding a lambda that is intermediate! \n",
    "- Large Lambda ($\\lambda$): \n",
    "    - Might create **High Bias** problem because we will be underfitting this data since we will be having smaller coefficients. \n",
    "    - Moreover, underfitting means that random event aren't taken into account. The visual is that we will have similar wrong prediction.\n",
    "- Small Lambda ($\\lambda$): \n",
    "    - Might create **High Variance** problem since the closer lambda is to 0, the less the effect regurlization has. Hence, the coefficients will not be regularizated and it will have a bias to the data it was trained on.\n",
    "<img src=\"../images/Image7.png\" alt=\"Drawing\" style=\"width: 500px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curves \n",
    "- As you add more observation, the error increases because the model can fit better on small datasets rather than large datasets\n",
    "- The more data, the more the cost function of the training and cross-validation are closer together\n",
    "- **If a learning algorithm is suffering from high bias**\n",
    "    - More training data does not help much.\n",
    "    - High bias means that the data suffers from underfitting. \n",
    "    - Simple models don't consider all possible outcomes so we will constantly get the wrong result, hence, more data points won't do anything. \n",
    "<img src=\"../images/Image8.png\" alt=\"Drawing\" style=\"width: 300px;\" >\n",
    "<img src=\"../images/Image9.png\" alt=\"Drawing\" style=\"width: 300px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deciding What to Do Next Revisited\n",
    "- **If a learning algorithm suffers from high variance**\n",
    "    - More training data will help\n",
    "    - Since variance means that we overfit the data, variance means that our model is producing different results for different data.\n",
    "    - Hence, if provide a larger dataset, it will learn more of the nuances in the data.\n",
    "<img src=\"../images/Image10.png\" alt=\"Drawing\" style=\"width: 350px;\">\n",
    "<img src=\"../images/Image11.png\" alt=\"Drawing\" style=\"width: 350px;\">\n",
    "The error continues to decrease because we can fit the data more as you begin to understand the data more\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extras\n",
    "**If you have HIGH VARIANCE PROBLEM:**\n",
    "- You can get more training examples\n",
    "- Try smaller sets of features (bc you are overfitting)\n",
    "- Try increasing lambda, so you can not overfit the training set as much. The higher the lambda, the more the regularization applies. \n",
    "\n",
    "**If you have HIGH BIAS PROBLEM:**\n",
    "- Try getting additional features\n",
    "- Try adding polynomial features\n",
    "- Try decreasing lambda, so you can try to fit the data better. The lower the lambda, the less the regularization applies.\n",
    "\n",
    "**Model Complexity Effects:**\n",
    "- Lower-order polynomials (low model complexity) have high bias and low variance. In this case, the model fits poorly consistently.\n",
    "- Higher-order polynomials (high model complexity) fit the training data extremely well and the test data extremely poorly. These have low bias on the training data, but very high variance.\n",
    "- In reality, we would want to choose a model somewhere in between, that can generalize well but also fits the data reasonably well.\n",
    "\n",
    "\n",
    "**Overfitting in Machine Learning**\n",
    "- Overfitting refers to a model that models the training data too well.\n",
    "- Overfitting happens when a model learns the detail and noise in the training data to the extent that it negatively impacts the performance of the model on new data. This means that the noise or random fluctuations in the training data is picked up and learned as concepts by the model. The problem is that these concepts do not apply to new data and negatively impact the models ability to generalize.\n",
    "\n",
    "**Underfitting in Machine Learning**\n",
    "- Underfitting refers to a model that can neither model the training data nor generalize to new data.\n",
    "- An underfit machine learning model is not a suitable model and will be obvious as it will have poor performance on the training data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
