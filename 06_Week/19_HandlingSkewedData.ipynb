{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Skewed Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Metrics for Skewed Data\n",
    "<img src=\"../images/Confusion-Matrix.png\" alt=\"Drawing\" style=\"width: 500px;\">\n",
    "\n",
    "- Just improving the classification isn't an optimally way to check the power of the model. \n",
    "    - For example, if accuracy is improved from 99.2 to 99.5, are we making better prediction or are we simply predicting 0 more times.\n",
    "        - If most of the values are skewed (e.g. data indicates that people will have cancer), predicting people will have cancer without any thought-process behind can led to a higher accuracy.\n",
    "- We create a Prediction/Actual box (in a binary problem, it would be a 2 by 2. \n",
    "\n",
    "- **Classifier Accuracy**\n",
    "    - (true positives + true negatives) / (total examples)\n",
    "    - This is a good way of measuring UNLESS the data is skewed into one direction\n",
    "    - If the data is skewed, we don't really know if the model is good or we simply are predicting the model to be more like the skewed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trading Off Precision and Recall\n",
    "\n",
    "- Another method to use is **Recall/Precision**:\n",
    "    - **Precision**: Out of all the patient that we predicted that have cancer (or 1), what fraction actually have cancer?\n",
    "        - TRUE POSITIVES/PREDICTED POSITIVES -> TRUE POSITIVES/(TRUE POSITIVES + FALSE POSITIVES)\n",
    "        - Using the box, this is represented as row 1\n",
    "    - **Recall**: Out of all the patient that actually have cancer (or 1), what fraction did we correctly detect as having cancer?\n",
    "        - TRUE POSITIVES/ACTUAL POSITIVES -> TRUE POSITIVES/(TRUE POSITIVES + FALSE NEGATIVES)\n",
    "        - Using the box, this is represented as col. 1 \n",
    "\n",
    "- In cancer case, we can change the treshold of 0.5 to 0.7\n",
    "- If we do this, we will have a higher precision and a lower recall.\n",
    "- The tradeoff is the precision looks at the prediciton. How will did we predict. So even if we didn't predict ALL the values, if we did well on the predicitons we made, we have a good precision.\n",
    "- However, recall looks at all the actual values (that are 1) and see how well we predicted out of those. IT doesn't care if we, say predicted all the values are 1, bc it would return a high recall score since we predicted 1 to most of the actual 1's\n",
    "- It's NOT a good way to find the average of both scores!\n",
    "- A better way to evaluate these scores is to use a **F-SCORE or F1 Score**\n",
    "- **F-Score** falls btw 0 and 1, 0 being the worst and 1 being the best\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
