{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating a Learning Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating a Hypothesis\n",
    "\n",
    "- It doesn't matter much that your parameters fit the dataset if it doesn't fit the training set.\n",
    "- The dataset is broken into three parts: training set, cross-validation set, and testing set.\n",
    "- An estimate of how the dataset are broken: 60%, 20%, 20%\n",
    "- We can now calculate three separate error values for the three different sets using the following method:\n",
    "    - Optimize the parameters, $\\theta$ using the training set for each polynomial degree.\n",
    "    - Find the polynomial degree d with the least error using the cross validation set.\n",
    "    - Estimate the generalization error using the test set with Jtest($\\theta_d$), (d = theta from polynomial with lower error);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection and Train/Test/Validation Set\n",
    "- **Training phase:** \n",
    "    - Present your data from a \"gold standard\" and train your model by pairing the input with expected output.\n",
    "- **Validation/Test phase:** \n",
    "    - In order to estimate how well your model has been trained (that is dependent upon the size of your data, the value you would like to predict, input etc) and to estimate model properties (mean error for numeric predictors, classification errors for classifiers, recall and precision for IR-models etc.)\n",
    "- **Application phase:** \n",
    "    - Applying your freshly-developed model to the real-world data and get the results. Since you normally don't have any reference value in this type of data (otherwise, why would you need your model?), you can only speculate about the quality of your model output using the results of your validation phase.\n",
    "The validation phase is often split into two parts:\n",
    "    - In the first part you just look at your models and select the best performing approach using the validation data (=validation)\n",
    "    - Then you estimate the accuracy of the selected approach (=test).\n",
    "Hence the separation to 50/25/25.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
