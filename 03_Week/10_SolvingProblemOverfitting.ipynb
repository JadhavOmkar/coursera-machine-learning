{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving the Problem of Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Problem of Overfitting\n",
    "High Bias\n",
    "- If a model underfits, say that it doesn't do a good job in prediciting the outcome\n",
    "\n",
    "High Variance\n",
    "- Or we might have a model that predicts it quite accurate. The shortcoming is that it's overfitting the data\n",
    "\n",
    "Overfitting: \n",
    "- If we have too many features, the learned hypothesis may fit the training data set very well but fail to generalize to new examples \n",
    "- Overfitting can be an issue if we  a lot of features but not enough training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function\n",
    "- Options:\n",
    "    - Reduce the number of features\n",
    "    - Manually select which features to keep\n",
    "    - Model selection algorithm \n",
    "- Regularization\n",
    "    - Keep all the features, but reduce magnitude/values parameters \n",
    "    - Works well when we have a lot of features, each of which contributes a bit to predicting y\n",
    "    \n",
    "**NOTES**\n",
    "- Underfitting, or high bias, is when the form of our hypothesis function h maps poorly to the trend of the data. It is usually caused by a function that is too simple or uses too few features.\n",
    "- Overfitting, or high variance, is caused by a hypothesis function that fits the available data but does not generalize well to predict new data. It is usually caused by a complicated function that creates a lot of unnecessary curves and angles unrelated to the data.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized Linear/Logistic Regression\n",
    "    \n",
    "- We can make the parameters, $\\theta$, smaller.\n",
    "- For examples, if we have 4 features and it might be overfitting, we can reduce the latter two parameters (where they are practically 0) and we are left with a quadratic function. Thus, we have a more general model, one that isn't overfitting as much.\n",
    "- Lambda is the regularization parameter\n",
    "- Lambda can't be too large because it will eliminate all the parameters, which will cause the model to UNDERFIT\n",
    "- We've added two extra terms at the end to inflate the cost of $\\theta_3$ and $\\theta_4$. Now, in order for the cost function to get close to zero, we will have to reduce the values of $\\theta_3$ and $\\theta_4$ to near zero\n",
    "- We don't want to penalize theta $\\theta$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
