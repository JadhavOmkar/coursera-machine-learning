{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving the Problem of Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Problem of Overfitting\n",
    "- If a model underfits, say that it doesn't do a good job in prediciting the outcome, we call it \"High Bias\"\n",
    "- At another extreme, we might have a model that predicts it quite accurate. The shortcoming is that it's overfitting the data\n",
    "- We call this \"High Variance\"\n",
    "- Overfitting: If we have too many features, the learned hypothesis may fit the training data set very well but fail to generalize to new examples \n",
    "- Overfitting can be an issue if we  a lot of features bt enough training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function\n",
    "- Options:\n",
    "    - Reduce the number of features\n",
    "    - Manually select which features to keep\n",
    "    - Model selection algorithm \n",
    "- Regularization\n",
    "    - Keep all the features, but reduce magnitude/values parameters \n",
    "    - Works well when we have a lot of features, each of which contributes a bit to predicting y\n",
    "    \n",
    "** NOTES**\n",
    "- Underfitting, or high bias, is when the form of our hypothesis function h maps poorly to the trend of the data. It is usually caused by a function that is too simple or uses too few features.\n",
    "- Overfitting, or high variance, is caused by a hypothesis function that fits the available data but does not generalize well to predict new data. It is usually caused by a complicated function that creates a lot of unnecessary curves and angles unrelated to the data.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized Linear/Logistic Regression\n",
    "    \n",
    "- We can make the parameters, thetas, smaller.\n",
    "- For examples, if we have 4 features and it might be overfitting, we can reduce the latter two parameters (where they are practically 0) and we are left with a quadratic function. Thus, we have a more general model, one that isn't overfitting as much.\n",
    "- Lambda is the regularization parameter\n",
    "- Lambda can't be too large because it will eliminate all the parameters, which will cause the model to UNDERFIT\n",
    "- We've added two extra terms at the end to inflate the cost of θ3 and θ4. Now, in order for the cost function to get close to zero, we will have to reduce the values of θ3 and θ4 to near zero\n",
    "- We don't want to penalize theta 0 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
