{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Features\n",
    "- Can have multiple features\n",
    "- Linear regression with multiple variables is also known as \"multivariate linear regression\".\n",
    "- m = the number of training examples\n",
    "- n = the number of feature\n",
    "- h$θ_{1}$=$θ_{0}$+$θ_{1}$$x_{1}$+$θ_{2}$$x_{2}$+$θ_{3}$$x_{3}$+⋯+$θ_{n}$$x_{n}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent in Practice I\n",
    "- Theta are the parameters\n",
    "- Feature Scaling: The values that the multiple parameters can take. If their range, scale, are similar, the algorithm can be conducted quickly. \n",
    "- If you plot two parameters that differ in scaling, we can see in a coutour map that that the eclipse is quite narrow and will take a long time until it gets to the middle\n",
    "- * Fix *: This can be fixed by scaling it or dividing it by the maximum possibility     \n",
    "- You should try the feature into approximately from -1 to 1 (not required)\n",
    "- Thumb of rule: Andrew likes ranges from -3 to 3, and -1/3 to 1/3\n",
    "- **Mean normalization:** Replace xi with xi - mean to make features have approx. zero mean, and this is divided either by the number of ranges of numbers or the std. deviation\n",
    "- This is because θ will descend quickly on small ranges and slowly on large ranges, and so will oscillate inefficiently down to the optimum when the variables are very uneven.\n",
    "- Two techniques to help with this are feature scaling and mean normalization.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent in Practice II\n",
    "- Feature scaling involves dividing the input values by the range (i.e. the maximum value minus the minimum value) of the input variable, resulting in a new range of just 1. \n",
    "- Mean normalization involves subtracting the average value for an input variable from the values for that input variable resulting in a new average value for the input variable of just zero.\n",
    "- *Debugging* - How to make that gradient descent is working correctly\n",
    "- Kind of obvious, but J(θ) should decrease when increasing the number of iterations\n",
    "- If you see that the gradient descent is increasing, then we probably need to decrease the learning rate\n",
    "- For sufficient small alpha, J(θ) should decrease after iteration but don't choose the alpha to be **to small**\n",
    "- ** NOTE **: Use different alphas, and check which alpha fits best!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features and Polynomial Regression\n",
    "- Creating new variables. Examples, instead of using frontage and depth, you could use the area \n",
    "- You can fit better models by either squaring or cubicing the variables\n",
    "- One thing to keep in mind is that the ranges might eventually become too large \n",
    "- We can change the behavior or curve of our hypothesis function by making it a quadratic, cubic or square root function (or any other form).\n",
    " \n",
    "- ** NOTE**: Remember that x0 is 1 because when we create the matrix, we know that we are only talking about one item, say house, but the other features have different values because the increase at a different rate if it increases by one unit. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
