{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descents with Large Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning With Large Datasets\n",
    "- For the most of the time, large datasets provides a high-performing model. The more data it has, the more information it has seen, which will led to a model that has a higher predictive power.\n",
    "- However, there's always a cost:\n",
    "    - Cost of using large datasets is computational power\n",
    "- If we have 1,000,000,000 features, this will take a lot of computational power. Why not just a subset of the examples of say 1,000?\n",
    "- One way to check if the larger dataset will perform better than subset is to **plot a learning curve of range of values of m and verify that the algorithm has high variance when m is small.**\n",
    "- A high variance is favoried because a higher variance means that lower bias. With enough data, the bias issues will be overcome because the average will be computed.\n",
    "<img src=\"../images/LargeDatasets-1.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent\n",
    "**Linear regression with Gradient Descent**\n",
    "- The problem with gradient descent is computational power. When, m, the number of observations is large, it will have to sum through all the observations before calculating the gradient descent. (**\"Batch Gradient Descent\"**)\n",
    "    - <img src=\"../images/LargeDatasets-2.png\" alt=\"Drawing\" style=\"width: 300px;\"/>  <img src=\"../images/LargeDatasets-3.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "\n",
    "A more efficient method is the: **Stochastic Gradient Descent**\n",
    "\n",
    "ProcessL\n",
    "1. We have to randomly shuffle the dataset\n",
    "    - Instead of summing through all the data, a stochastic gradient descent calculates the gradient descent after each iteration\n",
    "    - However, this will create the gradient descent to be more noiser. Thus, we might not always reach the local optima.\n",
    "    - Because the data cannot be assured will reach the local minima, we would have to reiterate this through a for loop 1 to 10 times.\n",
    "    - However, if m is very large, we don't have to do reiterate a lot\n",
    "    - <img src=\"../images/LargeDatasets-4.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "Comparing the path a batch gradient descent versus the stochastic gradient descent:\n",
    "- Batch Gradient Descent:\n",
    "    - <img src=\"../images/LargeDatasets-5.png\" alt=\"Drawing\" style=\"width: 200px;\"/>\n",
    "- Stochastic Gradient Descent:\n",
    "    - <img src=\"../images/LargeDatasets-6.png\" alt=\"Drawing\" style=\"width: 200px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-Batch Gradient Descent\n",
    "- A more effective method is to implement a combination of the stochastic gradient descent and the batch gradient descent.\n",
    "    - This variable will be labeled b: B is the mini-batch size \n",
    "    - So, we will use b instead of m (the number of observations)\n",
    "    - The typical size of b would be between 2-100\n",
    "<img src=\"../images/LargeDatasets-7.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "<img src=\"../images/LargeDatasets-8.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "**Why choose Mini-Batch Gradient Descent over Stochastic Gradient Descent?**\n",
    "- Vectorization\n",
    "- Parallel computation is possible because of vectorization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent Convergence\n",
    "- As stochastic gradient descent is scanning through our training set, right before we have updated the thetas, let's compute how well our hypothesis is doing on the training examples\n",
    "<img src=\"../images/LargeDatasets-9.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "- Sometimes a smaller learning parameter can be better\n",
    "- The larger the data oberservations, we can have a smoother graph\n",
    "<img src=\"../images/LargeDatasets-10.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "- One good way for the model to reach the minimum, it would be to decrease the learning parameter over time. A pretty typically way is using the following image\n",
    "<img src=\"../images/LargeDatasets-11.png\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
