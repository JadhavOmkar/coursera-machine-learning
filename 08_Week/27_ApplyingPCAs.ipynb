{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruction from Compressed Representation \n",
    "- The following is the implementation of transforming our data (linear algebra) back to its original form\n",
    "    - We must use U reduce * z to get the X approximation matrix\n",
    "- <img src=\"../images/PCA-8.png\" alt=\"Drawing\" style=\"width: 500px;\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the Number of Principal Components \n",
    "\n",
    "The goal of the PCA is to:\n",
    "- Minimize the average squared projection error\n",
    "    - $1/m \\Sigma_{i=1}^m ||x^{(1)} - x_{approx}^{(1)}||^2$\n",
    "- Total variation in the data: \n",
    "    - $1/m \\Sigma_{i=1}^m ||x^{(1)}||^2$\n",
    "    - On average, how far are the training examples from the origin\n",
    "- Typically, choose $k$ to be smallest value so that:\n",
    "    - $(1/m \\Sigma_{i=1}^m ||x^{(1)} - x_{approx}^{(1)}||^2)$ / $(1/m \\Sigma_{i=1}^m ||x^{(1)}||^2)$\n",
    "- The goal is that our calculation will be less than 0.01 which indicates that 99% of our variance is retained. We want the averafe square projection to be really small!\n",
    "\n",
    "<img src=\"../images/PCA-10.png\" alt=\"Drawing\" style=\"width: 300px;\">\n",
    "\n",
    "There are two methods to calculate the K (use the image below)\n",
    "- The right-side equation computes by passing the K=1, and continue to increase K until we reach our desired goal. This method is simple but not computational efficient!\n",
    "- The left-side equation computes the S in the SVD!\n",
    "    - S is the eigenvalue (a diagnol matrix)\n",
    "    - Thus we compute the eigenvalue of its specific entry (say 3) by dividing amongst the sum of all the eigenvalue\n",
    "    - This method is the same as the method described above (left-side in the image)\n",
    "- <img src=\"../images/PCA-11.png\" alt=\"Drawing\" style=\"width: 400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advice for Applying PCA\n",
    "- **Supervised learning speedup**\n",
    "    - If you're doing image processing, and you have a 100 by 100 image, you really have 10000 pixel feature and this can take a lot of time\n",
    "    - <img src=\"../images/PCA-11.png\" alt=\"Drawing\" style=\"width: 400px;\">\n",
    "    - Thus, computing the PCA to map into a lower-dimension while keeping the variance intact can reduce the dimension which will speed the computation when training the data\n",
    "    - ONLY PERFORM THE PCA ON THE TRAINING SET AND NOT THE CV SET!\n",
    "- **Application of PCA**\n",
    "    - Compression\n",
    "        - Reduce memory/disk needed to store data\n",
    "        - Speed up the algorithm\n",
    "    - Visualization\n",
    "        - Works with k=2 or k=3\n",
    "- **Bad USE of PCA**\n",
    "    - PCA can also cause you to lose valuable information to prevent overfitting! This is not the worst method but regularizaiton is better. You will throw away the predictor value which loses information of the data!\n",
    "    \n",
    "<img src=\"../images/PCA-12.png\" alt=\"Drawing\" style=\"width: 400px;\"> \n",
    "\n",
    "Another suggestion is to not use PCA right away. You should work with the original RAW data and compute/train the model.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
