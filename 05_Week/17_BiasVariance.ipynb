{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias and Variance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnosing Bias vs. Variance\n",
    "- ** LEFT**: This is high bias problem. We are underfitting the model from the training to the cv or testing set. \n",
    "- ** RIGHT**: This is high variance problem. We are overfitting the model from the training to the cv or testing set. \n",
    "- ** BIAS**: The training and cv error will have high cost function\n",
    "- ** VARIANCE**: The training error will be low (you are fitting the training set well), and the cv error will be higher than the training set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization and Bias/Variance\n",
    "\n",
    "- When there's a large lambda: We tend to underfit the data\n",
    "- When there's a small lambda: We tend to overfit the data\n",
    "- NEED TO LOOK FOR AN INTERMEDIATE LAMBDA\n",
    "- LARGE LAMBDA: **HIGH BIAS** because we are underfitting (since we are changing the equation). Moreover, underfitting means that random event aren't taken into account. The visual is that we will have similar wrong prediction.\n",
    "- SMALL LAMBDA: **HIGH VARIANCE** because we are close to zero (almost adding no value, so the equation remains the same) so there will be overfitting. Moreover, overfitting means that we can accuratly find different results centered around the correct prediction. \n",
    "<img src=\"../images/Image7.png\" alt=\"Drawing\" style=\"width: 1000px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curves \n",
    " - As you add more observation, the error increases because the model can fit better on small datasets rather than large datasets\n",
    " - The more data, the more the cost function of the training and cross-validation are closer together\n",
    " - ** If a learning algorithm is suffering from high bias**, getting more training data WILL NOT by itself help much\n",
    " - This is because bias means that it suffers from underfitting. If create a simple model that doesn't consider all possible outcomes, we will constantly get the wrong result, hence, more data points won't do anything. \n",
    "<img src=\"../images/Image8.png\" alt=\"Drawing\" style=\"width: 500px;\" >\n",
    "<img src=\"../images/Image9.png\" alt=\"Drawing\" style=\"width: 500px;\">\n",
    "- There's a high error in the training set bc we are underfitting\n",
    "- Look at the graphs above. When we have a simple model (since that's when underfitting occurs), adding more observation still causes underfitting. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deciding What to Do Next Revisited\n",
    "- ** If a learning algorithm is suffering from high variance**, getting more training data WILL by itself help much\n",
    "     - This occurs because variance means that it overfits our model. Variance means that we will differernt results. Because the model overfits the training sample, the model will get differnt results throughout each prediction. As we get more data, we will be able to find an average.\n",
    "<img src=\"../images/Image10.png\" alt=\"Drawing\" style=\"width: 500px;\">\n",
    "<img src=\"../images/Image11.png\" alt=\"Drawing\" style=\"width: 500px;\">\n",
    "         - The error continues to decrease because we can fit the data more as you begin to understand the data more\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extras\n",
    "** If you have HIGH VARIANCE PROBLEM:**\n",
    "- You can get more training examples\n",
    "- Try smaller sets of features (bc you are overfitting)\n",
    "- Try increasing lambda, so you can not overfit the training set as much. The higher the lambda, the more the regularization applies. \n",
    "\n",
    "** If you have HIGH BIAS PROBLEM:**\n",
    "- Try getting additional features\n",
    "- Try adding polynomial features\n",
    "- Try decreasing lambda, so you can try to fit the data better. The lower the lambda, the less the regularization applies.\n",
    "\n",
    "**Model Complexity Effects:**\n",
    "- Lower-order polynomials (low model complexity) have high bias and low variance. In this case, the model fits poorly consistently.\n",
    "- Higher-order polynomials (high model complexity) fit the training data extremely well and the test data extremely poorly. These have low bias on the training data, but very high variance.\n",
    "- In reality, we would want to choose a model somewhere in between, that can generalize well but also fits the data reasonably well.\n",
    "\n",
    "\n",
    "**Overfitting in Machine Learning**\n",
    "- Overfitting refers to a model that models the training data too well.\n",
    "- Overfitting happens when a model learns the detail and noise in the training data to the extent that it negatively impacts the performance of the model on new data. This means that the noise or random fluctuations in the training data is picked up and learned as concepts by the model. The problem is that these concepts do not apply to new data and negatively impact the models ability to generalize.\n",
    "\n",
    "**Underfitting in Machine Learning**\n",
    "- Underfitting refers to a model that can neither model the training data nor generalize to new data.\n",
    "- An underfit machine learning model is not a suitable model and will be obvious as it will have poor performance on the training data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
