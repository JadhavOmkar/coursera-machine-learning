{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Function and Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function\n",
    "Forwardpropagation\n",
    "<img src=\"../images/Image6.png\" alt=\"Drawing\" style=\"width: 450px;\"/>\n",
    "- L = total number of layers in the network\n",
    "- sl = number of units (not counting bias unit) in layer l \n",
    "- K = number of output units/classes\n",
    "- \"Backpropagation\" is neural-network terminology for minimizing our cost function, just like what we were doing with gradient descent in logistic and linear regression. Our goal is to compute: $minΘ_{J}$\n",
    "- Gradient Descent is a great way to compute each layer. Using the parameters for each layer, we use the result from the previous layer multiplied by the theta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backprogation Algorithm\n",
    "- Using $y(t)$, compute $δ(L)=a(L)−y(t)$\n",
    "    - Where L is our total number of layers and a(L) is the vector of outputs of the activation units for the last layer. So our \"error values\" for the last layer are simply the differences of our actual results in the last layer and the correct outputs in y. To get the delta values of the layers before the last layer, we can use an equation that steps us back from right to left"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extras\n",
    "**Not DESCRIBED in Andrew Ng's Course**\n",
    "- http://datathings.com/blog/post/neuralnet/ (Some intuition behind forward and back propogation\n",
    "- We have a set of inputs and output(s)\n",
    "- We create some function, of whatever we think it is, and check the results of our function with the inputs and compare it the actual output(s)\n",
    "- We then use a cost function to check the results, and get some information on how well/bad our parameters in our function did\n",
    "- For example, if we are using a linear problem, we would change the parameters(or theta) for a better model to minimize the cost function \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
