{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropogation in Practice "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Note: Unrolling Parameters \n",
    "- Learning Algorithm\n",
    "    - Have initial parameters $Θ_{1}$, $Θ_{2}$, $Θ_{3}$\n",
    "    - Unroll to get initialTheta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Checking\n",
    "- ** Epsilon **: In mathematics, a small positive infinitesimal quantity, usually denoted epsilon or epsilon, whose limit is usually taken as epsilon->0.\n",
    "- Implement backprop to calculate for DVec\n",
    "- Implement numerical gradient check to compute gradApprox\n",
    "- Make sure they have similar values\n",
    "- Turn OFF Gradient Checking. Using backprop code for learning OR else your code will be running very slowly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Initialization\n",
    "- You can't initialize the parameters to 0 bc mathematically, you won't get the desired partial derivative \n",
    "- Thus, you have to use the random feature\n",
    "- Initializing all theta weights to zero does not work with neural networks. When we backpropagate, all nodes will update to the same value repeatedly. Instead we can randomly initialize our weights for our Θ matrices using the following method: <img src=\"../images/Image4.png\">\n",
    "- rand(x,y) is just a function in octave that will initialize a matrix of random real numbers between 0 and 1.\n",
    "- ** Putting in Together **\n",
    "1. Training a neural network\n",
    "    - Pick a network architecture - Meaning you must have to choose how much input, hidden, and output features you want in your algorithm\n",
    "    - Num. of input units is the dimension of features\n",
    "    - Num. of output units is the number of classes\n",
    "    - Reasonable default: 1 Hidden layer, or if >1 hidden layers, have some num. of hidden units in each layer\n",
    "2. Randomly initilize weights\n",
    "3. Implement forward propogagation\n",
    "4. Implement code to compute cost function\n",
    "5. Implement back prop. to compute the partial derivative\n",
    "6. Use Gradient Checking\n",
    "7. Use Gradient Descent or advanced optimization with backprop. to try to minimize as a function of parameters\n",
    "- Ideally, you want $hΘ(x(_i))$ ≈ y(i). This will minimize our cost function. However, keep in mind that J(Θ) is not convex and thus we can end up in a local minimum instead.\n",
    "<img src=\"../images/Image5.png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
